{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Implementing a Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn\n",
    "# Install seaborn (needed to plot confusion matrix) by uncommenting the above line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sigmoid_forward(a):\n",
    "    # calculates the sigmoid activation function\n",
    "    # a: pre-activation values\n",
    "    # returns: activated values\n",
    "    return 1.0 / (1.0 + 1/np.exp(a))\n",
    "\n",
    "def sigmoid_backward(grad_accum, a):\n",
    "    # grad_accum: the gradient of the loss function w.r.t to z\n",
    "    # a: the pre-activation values\n",
    "    # returns: the gradient of the loss w.r.t to the preactivation values, a\n",
    "    out = []\n",
    "    for i in range(len(grad_accum)-1):\n",
    "        out.append(grad_accum[i+1] *  np.multiply(a, (1.0 - a))[i])\n",
    "    return np.array(out).T # np.dot(grad_accum[1:], np.multiply(a, (1.0 - a)).T)\n",
    "\n",
    "def linear_forward(x, weight, bias):\n",
    "    # Computes the forward pass of the linear layer\n",
    "    # x: input of layer\n",
    "    # weight, bias: weights and bias of neural network layer\n",
    "    # returns: output of linear layer\n",
    "    x_out = []\n",
    "    for i_layer, (i_w, i_b) in enumerate(zip(weight, bias)):\n",
    "        x_out.append(np.dot(x, i_w) + i_b)\n",
    "    return np.array(x_out)\n",
    "\n",
    "def linear_backward(grad_accum, x, weight, bias):\n",
    "    #  Derivative of the linear layer w.r.t \n",
    "    # grad_accum: gradient of loss w.r.t function after linear layer\n",
    "    # returns dl_dw: gradient of loss w.r.t to weights \n",
    "    # returns dl_dx: gradient of loss w.r.t to input, x  \n",
    "    # return dl_dw, dl_dx \n",
    "\n",
    "    dl_dw = np.dot(np.insert(x, 0, 1, axis = 0), grad_accum).T\n",
    "    dl_dx = np.dot(grad_accum, np.insert(weight, 0, bias, axis = 1)).T\n",
    "    return dl_dw, dl_dx\n",
    "    \n",
    "def softmax_xeloss_forward(b, labels):\n",
    "    # Input parameters: \n",
    "    ## b: pre-activation \n",
    "    # calculates the softmax of the vector b\n",
    "    # calculates the cross entropy loss between the softmax of b and the labels \n",
    "    # returns: l\n",
    "    y_hat = np.exp(b-100000) / np.sum(np.exp(b-100000))\n",
    "    l = np.multiply(labels, np.log(y_hat).T)\n",
    "    return l\n",
    "    \n",
    "def softmax_xeloss_backward(yhat, labels):\n",
    "    # Input parameters:\n",
    "    # yhat: predictions of the neural network\n",
    "    # labels: target of the network\n",
    "    # returns: dl_db gradient of loss w.r.t to b\n",
    "    return yhat - labels\n",
    "    \n",
    "def data_load():\n",
    "    # load in the data provided in \"data/\"\n",
    "    # Unzip fashion_mnist.zip\n",
    "    # Unzipped manually because I wasn unsure if I could import zipfile for this assignment base on conditions on pdf.\n",
    "    x_train = pd.read_csv('./data/train.csv', header=None)\n",
    "    x_test = pd.read_csv('./data/test.csv', header=None)\n",
    "    \n",
    "    y_train = x_train.iloc[:,-1]\n",
    "    x_train = x_train.drop(x_train.columns[[-1]], axis = 1)\n",
    "    y_test = x_test.iloc[:,-1]\n",
    "    x_test = x_test.drop(x_test.columns[[-1]], axis = 1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def load_params():\n",
    "    alpha_weights = np.loadtxt('params/alpha1.txt', delimiter=',')\n",
    "    beta_weights = np.loadtxt('params/alpha2.txt', delimiter=',')\n",
    "    alpha_bias= np.loadtxt('params/beta1.txt', delimiter=',')\n",
    "    beta_bias = np.loadtxt('params/beta2.txt', delimiter=',')\n",
    "    return alpha_weights, beta_weights, alpha_bias, beta_bias\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    # convert categorical target features to one hot encoded data\n",
    "    labels = np.sort(y.unique())\n",
    "    y_encoded = pd.DataFrame(np.zeros((len(y), len(labels))), columns = labels.astype(str))\n",
    "    for i in labels:\n",
    "        y_encoded.loc[y == i, str(i)] = 1\n",
    "    return y_encoded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(batchsize=1 , eta = 0.01, num_epochs=100, h = 256, init='default'):\n",
    "    X_train, y_train, X_test, y_test = data_load()  \n",
    "    \n",
    "    y_train = one_hot_encode(y_train) \n",
    "    y_test = one_hot_encode(y_test) \n",
    "\n",
    "    if init == 'default':\n",
    "        alpha_weights, beta_weights, alpha_bias, beta_bias = load_params()\n",
    "    elif init=='zeros':\n",
    "        # initialize weights and biases to 0\n",
    "        alpha_weights = np.zeros(h * len(X_train.columns))\n",
    "        beta_weights = np.zeros(h * len(y_train.columns))\n",
    "        alpha_bias = np.zeros(h)\n",
    "        beta_bias = np.zeros(len(y_train.columns))\n",
    "    elif init=='ones':\n",
    "        # initialize weights and biases to 1\n",
    "        alpha_weights = np.ones(h * len(X_train.columns))\n",
    "        beta_weights = np.ones(h * len(y_train.columns))\n",
    "        alpha_bias = np.ones(h)\n",
    "        beta_bias = np.ones(len(y_train.columns))\n",
    "    elif init=='random':\n",
    "        # initialize weights and biases to random values between -1 and 1\n",
    "        alpha_weights = np.random.uniform(-1, 1, h * len(X_train.columns))\n",
    "        beta_weights = np.random.uniform(-1, 1, h * len(y_train.columns))\n",
    "        alpha_bias = np.random.uniform(-1, 1, h)\n",
    "        beta_bias = np.random.uniform(-1, 1, len(y_train.columns))\n",
    "\n",
    "        \n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    acc_list = []\n",
    "    \n",
    "    \n",
    "    for epoch in (range(num_epochs)):\n",
    "        #continue\n",
    "        error = []\n",
    "        y_pred_train = np.zeros_like(y_train)\n",
    "        # Iterate over batches of data        \n",
    "        for i in range(batchsize):\n",
    "    \n",
    "            # do not shuffle data\n",
    "\n",
    "            # select batch\n",
    "            X_sample = X_train[i: i + 1]\n",
    "            y_sample = y_train[i: i + 1]\n",
    "\n",
    "            \n",
    "            ######## FORWARD \n",
    "            # Linear -> Sigmoid -> Linear -> Softmax\n",
    "            forward1 = linear_forward(X_sample, alpha_weights, alpha_bias)\n",
    "            activation1 = sigmoid_forward(forward1)\n",
    "            forward2 = linear_forward(activation1.T, beta_weights, beta_bias)\n",
    "            y_pred_train[i:i+1] = (np.exp(forward2) / np.sum(np.exp(forward2))).T\n",
    "            error.append(softmax_xeloss_forward(forward2, y_sample))\n",
    "            \n",
    "            ######## BACKWARD \n",
    "            grad_softmax = softmax_xeloss_backward(y_pred_train[i], y_sample)\n",
    "            grad_beta, grad_z = linear_backward(grad_softmax, activation1, beta_weights, beta_bias)\n",
    "            grad_sigmoid = sigmoid_backward(grad_z, forward1)\n",
    "            grad_alpha, grad_x = linear_backward(grad_sigmoid, X_sample.to_numpy().T, alpha_weights, alpha_bias)\n",
    "\n",
    "            ######## UPDATE\n",
    "            alpha_weights = alpha_weights - eta * grad_alpha[:,1:]\n",
    "            alpha_bias = alpha_bias - eta * grad_alpha[:,0]\n",
    "            beta_weights = beta_weights - eta * grad_beta[:,1:]\n",
    "            beta_bias = beta_bias - eta * grad_beta[:,0]\n",
    "\n",
    "\n",
    "\n",
    "        # store average training loss for the epoch\n",
    "        error.append(softmax_xeloss_forward(forward2, y_sample))\n",
    "        train_loss_list.append(np.mean(error))\n",
    "        \n",
    "        # calculate test predictions and loss\n",
    "        error = []\n",
    "        y_pred_test = np.zeros_like(y_test)\n",
    "        # Iterate over batches of data        \n",
    "        for i in range(batchsize):\n",
    "    \n",
    "            # do not shuffle data\n",
    "\n",
    "            # select batch\n",
    "            x_sample = X_test[i: i + 1]\n",
    "            y_sample = y_test[i: i + 1]\n",
    "#             alpha_weights = alpha_weights[i:((i + 1) * 784)]\n",
    "#             beta_weights = beta_weights[i:((i + 1) * 256)]\n",
    "            \n",
    "            ######## FORWARD \n",
    "            forward1 = linear_forward(X_sample, alpha_weights, alpha_bias)\n",
    "            activation1 = sigmoid_forward(forward1)\n",
    "            forward2 = linear_forward(activation1.T, beta_weights, beta_bias)\n",
    "            y_pred_test[i:i+1] = (np.exp(forward2) / np.sum(np.exp(forward2))).T\n",
    "            error.append(softmax_xeloss_forward(forward2, y_sample))\n",
    "        error.append(softmax_xeloss_forward(forward2, y_sample))\n",
    "        test_loss_list.append(np.mean(error))\n",
    "        \n",
    "        # calculate test accuracy\n",
    "        total = len(y_test)\n",
    "        correct = (y_pred_test == y_test).sum()\n",
    "        acc_list.append(correct/float(total))\n",
    "    # return train_loss_list, test_loss_list, as well as test and train predictions\n",
    "    #pass\n",
    "    return train_loss_list, test_loss_list, y_pred_train, y_pred_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss, testing loss as a function of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/zlyt_0jn1cj4plrbb7kf80d80000gn/T/ipykernel_45340/615367044.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "  y_hat = np.exp(b-100000) / np.sum(np.exp(b-100000))\n",
      "/var/folders/9_/zlyt_0jn1cj4plrbb7kf80d80000gn/T/ipykernel_45340/615367044.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + 1/np.exp(a))\n",
      "/var/folders/9_/zlyt_0jn1cj4plrbb7kf80d80000gn/T/ipykernel_45340/615367044.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return 1.0 / (1.0 + 1/np.exp(a))\n",
      "/var/folders/9_/zlyt_0jn1cj4plrbb7kf80d80000gn/T/ipykernel_45340/615367044.py:5: RuntimeWarning: overflow encountered in true_divide\n",
      "  return 1.0 / (1.0 + 1/np.exp(a))\n",
      "/var/folders/9_/zlyt_0jn1cj4plrbb7kf80d80000gn/T/ipykernel_45340/615367044.py:13: RuntimeWarning: overflow encountered in multiply\n",
      "  out.append(grad_accum[i+1] *  np.multiply(a, (1.0 - a))[i])\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss, train_pred, test_pred = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(yhat, y, title = '[Training or Test] Set'):\n",
    "\n",
    "    pred_train = np.argmax(yhat, axis=1)\n",
    "    true_train = np.argmax(y, axis=1)\n",
    "    print(true_train.shape)\n",
    "    conf_train = np.zeros((10,10))\n",
    "    for i in range(len(y)):\n",
    "        conf_train[ true_train[i], pred_train[i] ] += int(1)\n",
    "        \n",
    "    sns.heatmap(conf_train, annot=True, fmt='.3g')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Title')\n",
    "    plt.show()\n",
    "# plot_confusion(yhat_train, y_train, title = \"Training Set\")\n",
    "# plot_confusion(yhat_test, y_test, title = \"Test Set\")\n",
    "#yhat: predictions\n",
    "#y: one-hot-encoded labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct and Incorrect Classification Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(vector, out_f_name, label=None):\n",
    "    \"\"\"\n",
    "    Takes a vector as input of size (784) and saves as an image\n",
    "    \"\"\"\n",
    "    image = np.asarray(vector).reshape(28, 28)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    if label:\n",
    "        plt.title(label)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{out_f_name}.png', bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plot_image function to display samples that are correctly and incorrectly predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect Of Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Initialization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2646d046b8e4716260f110cd84fd15ddf7abd46056d88656aca2ddf716f73663"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
